---
title: "Final Exam"
author: "Cody Frisby"
date: "April 26, 2016"
output: pdf_document
---

## 1  

- The model for this experiment can be described by $$y_{ijkl} = \mu + \tau_i + \beta_j + \gamma_k + (\tau\beta)_{ij} + (\tau\gamma)_{ik} + (\beta\gamma)_{jk} + (\tau\beta\gamma)_{ijk} + \varepsilon_{ijkl}\left\{
        \begin{array}{ll}
            i = 1, 2 \\
            j = 1, 2 \\
            k = 1, 2 \\
            l = 1, 2, 3, 4
        \end{array}
    \right.
    $$
where it is assumed that A, B, and C are fixed and $\varepsilon_{ijkl}$ are NID(0, $\sigma^2$). 

- The main and interaction effects are 

```{r, echo=F}
############ NUMBER 1 ############
# 2^3 design 
n <- 4; y <- c(425,426,1118,1283,1203,1396,1670,1807)
# build the design matrix
A <- c(-1,1,-1,-1,1,1,-1,1); B <- c(-1,-1,1,-1,1,-1,1,1)
C <- c(-1,-1,-1,1,-1,1,1,1);AB <- A*B; AC <- A*C; BC <- B*C; 
ABC <- A*B*C; dm <- cbind(A,B,C,AB,AC,BC,ABC)
con <- apply(dm, 2, FUN = function(x) sum(x*y)) 
effects <- con/16
# effects
id <- c("(1)", "a", "b", "c", "ab", "ac", "bc", "abc")
ss <- round((con^2)/32, 3) # sum of squares
# note, sst and sse cannot be estimated percisely
sst <- 466150; sse <- sst - sum(ss); mse <- sse/24 
f <- round(ss/mse,4); p <- round(pf(f,1,24, lower.tail = FALSE),5)
df1 <- data.frame(Factor = colnames(dm), effect = effects, 
                  Df = c(rep(1,7)),SS = ss, MS = ss, F0 = f, p)
rownames(df1) <- NULL
df3 <- df1[-2]
df3 <- rbind(df3, data.frame(Factor = c("Error", "Total"), Df = c(24,31), 
        SS = c(sse, sst), MS = c(mse, NA), F0 = c(NA, NA), 
        p = c(NA, NA)))
```

`r knitr::kable(df1[1:2])` 

- And the ANOVA table `r knitr::kable(df3)`  

```{r, echo=FALSE, message=F, warning=F, dpi=200, fig.align='center'}
#tmp <- qqnorm(abs(effects), ylim = c(-1,205), main = "Normal Plot of the Effects")
#qqline(abs(effects))
#text(tmp$x, tmp$y, names(effects), pos=1, cex=0.6)
```

- Here, I started to construct interaction plots of the effects of A, B, and C  where I seperated the two plots into A low and A high, but seeing as there is no significant interaction effect from A, I will just plot the interaction between B and C.

```{r, echo=FALSE, message=F, warning=F, dpi=200, fig.align='center', fig.height=4.5}
d <- data.frame(dm[,1:3], y)
d$y <- d$y/4
#par(mfrow=c(1,2))
#with(d[d$A == -1,], interaction.plot(B, C, y, col=2:3, main = "A low"))
#with(d[d$A == 1,], interaction.plot(B, C, y, col=2:3, main = "A high"))
with(d, interaction.plot(B, C, y, col=2:3))
```

There does not appear to be any evidence of a significant interaction effect from A.  There is strong evidence that B & C have the largest effect on the response.  And I would set B and C at their respective high levels to maximize the response.  We could also set A at "high" since its effect is positive.  


## 2  

- A model that relates y to $x_1$ and $x_2$ that is supported by the design of this experiment is $$y_{ijk} = \mu + \tau_i + \beta_j + (\tau\beta)_{ij} + \varepsilon_{ijk}\left\{
        \begin{array}{ll}
            i = 1, 2, 3 \\
            j = 1, 2, 3 \\
            k = 1, 2
        \end{array}
    \right.
    $$
where the errors are assumed NID(0, $\sigma^2$).  Also, the addition of a third factor level allows us to model the relationship between the response and design factors as quadratic.  

```{r, echo=FALSE, message=F, warning=F}
########### NUMBER 2 ###########
y <- c(86.3,84,85.8,86.1,85.2,87.3,88.5,87.3,89,89.4,
       89.9,90.3,89.1,90.2,101.3,91.7,93.2,98.7)
temp <- factor(c(rep("low",6), rep("med",6), rep("high",6)))
pressure <- factor(rep(c(250,260,270), 6))
df2 <- data.frame(y, temp, pressure)
df <- data.frame(y, temp = c(rep(-1,6), rep(0,6), rep(1,6)), 
                 pressure = rep(c(-1,0,1), 6))
fit <- aov(y ~ .*., df2)
# fitting the linear model
df$temp2 <- df$temp^2
df$pressure2 <- df$pressure^2
library(dplyr)
y <- df$y
y.i <- df %>%
  group_by(temp) %>%
  summarise_each(funs(sum), y)
y.j <- df %>%
  group_by(pressure) %>%
  summarise_each(funs(sum), y)
yij. <- df %>%
  group_by(temp, pressure) %>%
  summarise_each(funs(sum), y)
y.. <- sum(y)
a <- 3; b <- 3; n <- 2; N <- a*b*n
ssa <- (1/(b*n))*sum(y.i$y^2) - y..^2/N 
ssb <- (1/(a*n))*sum(y.j$y^2) - y..^2/N
sssub <- (1/n)*sum(yij.$y^2) - y..^2/N
ssab <- sssub - ssa - ssb
sst <- round(sum(y^2) - y..^2/N, 3)
sse <- sst - ssab - ssa - ssb
```

```{r, echo=FALSE}
# now to add squared terms
row.sums <- df %>%
  group_by(temp, pressure) %>%
  summarise_each(funs(sum), y)
y <- row.sums$y
id <- c("00", "01", "02", "10", "11", "12", "20", "21", "22")
#data.frame(id, y) # these are our summaries..
#... now to develop the matrix
Al <- c(rep(-1,3), rep(0,3), rep(1,3))
Bl <- rep(c(-1,0,1),3)
Aq <- c(rep(1,3), rep(-2,3), rep(1,3))
Bq <- rep(c(1,-2,1), 3)
AlBl <- Al*Bl; AlBq <- Al*Bq; AqBl <- Aq*Bl; AqBq <- Aq*Bq
dm <- cbind(Al,Bl,Aq,Bq,AlBl,AlBq,AqBl,AqBq)
rownames(dm) <- id
m.effects <- apply(dm,2, FUN = function(d) sum(d*y)/2) # effects
# now for the sum of squares...
C <- apply(dm, 2, function(x) sum(x^2)) #contrasts
ss <- round(2*(m.effects^2)/C, 3) # sum of squares
ss2 <- round(cbind(ssa,ssb,ssab,sse,sst), 3)
sse <- round(sst - sum(ss),3); mse <- round(sse/9, 3)
```


```{r, echo=FALSE}
effects <- rbind(m.effects)
rownames(effects) <- "effects"
colnames(effects) <- c("   Al", "   Bl", "   Aq", "   Bq", "   AlBl",
                       "   AlBq", "   AqBl", "   AqBq")
```

- The linear and quadradic effects of our model, where A = **temp** and B = **pressure**, are `r knitr::kable(effects)`  

- ANOVA table is displayed below

```{r, echo=FALSE}
fit <- aov(y ~ .*., df2)
t <- anova(fit)
# first ANOVA table
ms <- round(t[[3]], 3); f <- round(t[[4]], 3); p <- round(t[[5]], 4)
```

|Source|SS|Df|MS|$F_0$| P-Value |
|:------|:------|:-----:|:-------|:------:|------:|
|temp|$`r ss2[1]`$ |`r a-1`|$`r ms[1]`$|$`r f[1]`$ |$`r p[1]`$ |
|pressure|$`r ss2[2]`$| `r b-1`|$`r ms[2]`$|$`r f[2]`$|$`r p[2]`$|
|temp:pressure|$`r ss2[3]`$|$4$|$`r ms[3]`$|$`r f[3]`$|$`r p[3]`$|
|Error|$`r ss2[4]`$| 9 |$`r ms[4]`$|          
|Total|$`r ss2[5]`$| `r (a*b*n)-1`|  

and we conclude all effects are significant.  Now let's take a look at the ANOVA with linear and quadratic effects.  

```{r, echo=FALSE}
# second ANOVA table
ms <- ss/1; f <- round(ms/mse, 3)
d.f <- a*b*(n-1)
p <- round(pf(ss/mse, 1, d.f, lower.tail = FALSE), 3)
```

|Source| Sum Sq | Df | Mean Sq |F value|Pr(>F)|
|:------|:------|:-----:|:---------|:------:|------:|
|temp|$`r ss[1]`$| 1 |$`r ss[1]`$|$`r round(ss[1]/mse,4)`$|$`r p[1]`$|
|pressure|$`r ss[2]`$| 1 |$`r ss[2]`$|$`r round(ss[2]/mse,4)`$|$`r p[2]`$|
|$temp^2$|$`r ss[3]`$ | 1 | $`r ss[3]`$|$`r round(ss[3]/mse,4)`$|$`r p[3]`$|
|$pressure^2$|$`r ss[4]`$| 1 |$`r ss[4]`$|$`r round(ss[4]/mse,4)`$|$`r p[4]`$|
|temp:pressure|$`r ss[5]`$| 1 |$`r ss[5]`$|$`r round(ss[5]/mse,4)`$|$`r p[5]`$|
|$pressure^2:temp$|$`r ss[6]`$ | 1 |$`r ss[6]`$|$`r round(ss[6]/mse,4)`$|$`r p[6]`$| 
|$temp^2:pressure$|$`r ss[7]`$| 1 |$`r ss[7]`$|$`r round(ss[7]/mse,4)`$|$`r p[7]`$|
|$pressure^2:temp^2$|$`r ss[8]`$| 1 |$`r ss[8]`$|$`r round(ss[8]/mse,4)`$|$`r p[8]`$|
|Error|$`r sse`$| 9 | $`r mse`$ | | |
|Total|$`r sst`$| 17 | | | |

And we conclude that all effects are significant *except* $A_Q$, $A_LB_Q$, and $A_QB_Q$, at $\alpha = 0.05$.  

- There doesn't appear to be much evidence for curvature with temperature.  We can most likely estimate temp's effect effectively with only linear terms, thereby removing all quadratic temp terms.  Looking at an interaction plot we can see that there is some curvature with pressure and definite interaction between pressure and temperature.  The effect from pressure, `r ss2[2]`, can be dicected into its linear and quadratic components.  The proportion linear is $\frac{`r ss[2]`}{`r ss2[2]`} = `r ss[2]/ss2[2]`$. And the proportion quadratic would be, `r 1 - ss[2]/ss2[2]`.  At this point it would be up to the engineer to decide whether or not they are ok with only a linear term for pressure or if they feel like they need the quadratic term in the model, I would lean towards including it.  Additionaly, for the interaction term, $\frac{`r ss[5]`}{`r ss2[3]`} = `r ss[5]/ss2[3]`$ can be explained from the linear term.  Most of the remaining can be found in the **AqBl** term.  Again, we may need to consult with the scienctist/engineer to understand whether or not previous theory indicates this type of interaction or if they are comfortable excluding/including this term from/in the model since $A_Q$ is not significant by itself. Below, looking at the interaction plot provides some insight to the above analysis regarding linear and quadratic effects and their interactions. 

```{r, echo=FALSE, message=F, warning=F, dpi=200, fig.align='center'}
with(df, interaction.plot(temp, pressure, y, col = 2:4))
```


## 3  

- The model is $$y_{ij} = \mu + \tau_i + \varepsilon_{ij}\left\{
        \begin{array}{ll}
            i = 1, 2, 3 \\
            j = 1, 2,..., 11
        \end{array}
    \right.
    $$ where the errors and $\tau_i$ are assumed to be independent random variables that are normally distributed with mean zero and where the variance of any observation is given by $V(y_{ij}) = \sigma_\tau^2 + \sigma^2$.  

```{r, echo=FALSE}
flavor1 <- c(924,876,1150,1053,1041,1037,1125,1075,1066,977,886)
flavor2 <- c(891,982,1041,1135,1019,1093,994,960,889,967,838)
flavor3 <- c(817,1032,844,841,785,823,846,840,848,848,832)
df <- stack(data.frame(flavor1, flavor2, flavor3))
names(df) <- c("y", "flavor")
df$flavor <- as.factor(df$flavor)
fit <- aov(y ~ flavor, df)
```

- Yes, the variability of flavor melt time is significant.  Table 6 below contains the ANOVA results of this experiment.  

```{r, echo=FALSE, warning=FALSE, message=FALSE}
msa <- summary(fit)[[1]][1,3]; mse <- round(summary(fit)[[1]][2,3],4); n <- 11
sig.tau <- round((msa-mse)/n,4)
library(pander)
pander(anova(fit))
```

- The point estimate for $\hat \sigma_\tau^2$ is computed by $$\hat \sigma_\tau^2 = \frac{MS_{Treatments} - MS_E}{n} = \frac{`r msa` - `r mse`}{`r n`} = `r sig.tau`$$  

- The variance of any observation melt time is estimated by $$\hat \sigma_y^2 = \hat \sigma^2 + \hat \sigma_\tau^2 = `r mse` + `r sig.tau` = `r mse+sig.tau`$$  

```{r, echo=FALSE}
a <- 3
cv <- qchisq(c(0.025, 1 - 0.025), (n*a) - a, lower.tail = FALSE) 
stat <- ((n*a - a)*mse) / cv
```

- We can find a confidence interval for $\sigma^2$ by using the form $$\frac{(N-a)MS_E}{\chi_{\alpha/2, N-a}} \leq \sigma^2 \leq \frac{(N-a)MS_E}{\chi_{1 - (\alpha/2), N-a}}$$  and the interval $$[`r stat`]$$ is the 95% confidence interval for $\sigma^2$.  

```{r, echo=FALSE}
cv2 <- qf(c(0.025, 1 - 0.025), a-1, (a*n) - 1, lower.tail = FALSE)
L.U <- (1/n)*(((msa/mse)*(1/cv2)) - 1)
l.u <- L.U/(1 + L.U)
```

- To find a 95% confidence interval for $\frac{\sigma_\tau^2}{\sigma_\tau^2 + \sigma^2}$ we find L and U  $$L = \frac{1}{n}(\frac{MS_{Treatments}}{MS_E}\frac{1}{F_{\alpha/2,a-1,N-a}} - 1)$$ $$U = \frac{1}{n}(\frac{MS_{Treatments}}{MS_E}\frac{1}{F_{1-\alpha/2,a-1,N-a}} - 1)$$ and the interval is $$\frac{L}{1 + L} \leq \frac{\sigma_\tau^2}{\sigma_\tau^2 + \sigma^2} \leq \frac{U}{1 + U}$$ and plugging into this formula we get $$[`r l.u`]$$ and we conclude that variability between ice creams accounts for between 15.8 and 97.8 percent of the variability observed in melt times.  

## 4  

- The model is $$y_{ijk} = \mu + \alpha_i + \gamma_j + (\alpha\gamma)_{ij} + \varepsilon_{ijk}\left\{
        \begin{array}{ll}
            i = 1, 2,..., a \\
            j = 1, 2,..., b \\
            k = 1, 2,..., n
        \end{array}
    \right.
    $$ where the $\alpha_i$ are fixed effecs such that $\sum \alpha_i = 0$ and $\gamma_j, (\alpha\gamma)_{ij}$, and $\varepsilon_{ijk}$ are uncorrelated random variables have zero mean and variances: $V(\gamma_j) = \sigma_\gamma^2, V[(\alpha\gamma)_{ij}] = \sigma_{\alpha\gamma}$, and  $V(\varepsilon_{ijk}) = \sigma^2$

```{r, echo=FALSE}
###########  NUMBER 4 ##############
# unrestricted mixed model for number 4
ssa <- 39.27; ssb <- 3935.96; ssab <- 48.51; sse <- 30.67 
sst <- 4054.4
a <- 3; b <- 10; n <- 3
msa <- ssa/(a-1); msb <- ssb/(b-1) 
msab <- ssab/((a-1)*(b-1)); mse <- sse/(a*b*(n-1))
f.op <- round(msa/msab,4); f.part <- round(msb/msab,4) 
f.inter <- round(msab/mse,4)
p.op <- round(1 - pf(f.op, a-1, (a-1)*(b-1)),5)
p.part <- round(1 - pf(f.part, b-1, (a-1)*(b-1)),5)
p.inter <- round(1 - pf(f.inter, (a-1)*(b-1), a*b*(n-1)),5)
```

- And computing the remaining ANOVA table using the unrestricted model we get  

|Source|SS|Df|MS|$F_0$| P-Value |
|:------|:------|:-------|:-------|:------|:-------|
|Operator|$`r ssa`$ |`r a-1`|$`r msa`$| $`r f.op`$ |$`r p.op`$ |
|Part|$`r ssb`$| `r b-1`|$`r msb`$ |$`r f.part`$|$`r p.part`$|
|Operator:Part|$`r ssab`$| `r (b-1)*(a-1)`|$`r msab`$ |$`r f.inter`$|$`r p.inter`$|
|Error|$`r sse`$| `r a*b*(n-1)` |$`r mse`$|          
|Total|$`r sst`$| `r (a*b*n)-1`| 

where $$F_0 = \frac{MS_B}{MS_{AB}}$$
and we conclude that all the effects are significant.  

```{r, echo=FALSE}
y1 <- 34.9; y2 <- 36.5; y3 <- 36
se <- sqrt(msab/(b*n))
t <- qt(0.975, a*b*n - 1)
```

- The effect of operator is statistically significant, and we compute the confidence interval for the estimate of each operator using the formula $$\sqrt{\frac{MS_{AB}}{bn}} = \sqrt{\frac{`r msab`}{`r b*n`}} = `r se`$$ for the estimate of the standard error.  And the critical value for our assumed normal distribution being $t_{\frac{\alpha}{2},abn-1} = `r t`$.  And plugging into this formula we get $$y_i \pm `r se`(`r t`)$$  $$y_1: [`r y1 - (t*se)`, `r y1 + (t*se)`]$$ $$y_2: [`r y2 - (t*se)`, `r y2 + (t*se)`]$$ $$y_3: [`r y3 - (t*se)`, `r y3 + (t*se)`]$$  

```{r, echo=FALSE}
var.b <- (msb - msab)/(a*n)
var.tb <- (msab - mse)/n
```

- Yes, the effect of part is statistically significant.  And  $$E(MS_B) = \sigma^2 + n\sigma_{\alpha\gamma}^2 + an\sigma_\gamma^2$$ $$E(MS_{AB}) = \sigma^2 + n\sigma_{\alpha\gamma}^2$$ are the expected mean square estimates for **part** and **part:operator** interaction respectively.  The variance component for **part** is computed by $$\hat \sigma_\gamma^2 = \frac{MS_B - MS_{AB}}{an}$$ $$`r var.b` = \frac{`r msb` - `r msab` }{`r a` (`r n`)}$$ And the variance component $\sigma_{\tau\beta}^2$ is estimated by $$\hat \sigma_{\tau\beta}^2 = \frac{MS_{AB} - MS_E}{n} = `r var.tb`$$


## 5  

```{r, echo=FALSE}
##########  NUMBER 5  ##########
# nested model for number 5
ssa <- 1540; ssb <- 640; ssc <- 400; sse <- 680; sst <- 3260
SS <- c(1540,640,400,680,3260)
a <- 8; b <- 5; c <- 2; n <- 2
Factor <- c("Area", "Lawn(Area)", "Site(Lawn(Area))", "Error", "Total")
d <- c(a-1, a*(b-1), a*b*(c-1), a*b*c*(n-1), a*b*c*n - 1)
ms <- c(ssa/(a-1), ssb/(a*(b-1)), ssc/(a*b*(c-1)), sse/(a*b*c*(n-1)), NA)
t.2 <- data.frame(Factor,SS,df = d,ms)
f <- round(c(ms[1]/ms[2], ms[2]/ms[3], ms[3]/ms[4], NA, NA),4)
```

- This is a **3** stage nested design with lawn nested in area and sites nested in lawn with all factors being randomly selected from the population. 

- The statistical model is $$y_{ijkl} = \mu + \tau_i + \beta_{j(i)} + \gamma_{k(ij)} + \varepsilon_{(ijk)l}\left\{
        \begin{array}{ll}
            i = 1, 2,..., 8 \\
            j = 1, 2,..., 5 \\
            k = 1, 2 \\
            l = 1, 2
        \end{array}
    \right.
    $$  where $\varepsilon_{ijkl}$ is assumed NIC(0,$\sigma$).  It is also assumed that $Area_i$ ~ $iid N(0, \sigma_A^2)$ and $Lawn_{j(i)}$ ~ $iid N(0, \sigma_B^2)$ and $Site_{k(j(k))}$ ~ $iid N(0, \sigma_C^2)$.    

- Below I display a table containing the sum of squares, degrees of freedom, and mean squares from the experiment.  `r knitr::kable(t.2)`  

```{r, echo=F}
sig.tau <- (ms[1] - ms[2])/(b*c*n); sig.beta <- (ms[2] - ms[3])/(c*n)
sig.gam <- (ms[3] - ms[4])/n
```

- Since all factors are random, we extend similar ideas from 2 stage nested designs to this experiment where we have 3 stages, all random.  For the factor area we have $$\hat \sigma_\tau^2 = \frac{MS_{Area} - MS_{Lawn(Area)}}{bcn} = `r sig.tau`$$ and similarly for Lawn(Area) we have $$\hat \sigma_\beta^2 = \frac{MS_{Lawn(Area)} - MS_{Site(Lawn(Area))}}{cn} = `r sig.beta`$$ and finally for Site we have $$\hat \sigma_\gamma^2 = \frac{MS_{Site(Lawn(Area))} - MS_{E}}{n} = `r sig.gam`$$  


```{r, echo=F}
fc <- round(pf(f, d[1:3], d[-1], lower.tail = FALSE),5)
t.3 <- cbind(t.2, F0 = f, p = fc)
```

- F ratios are found similarly to mixed models, and I also run an F test for significance with $\alpha = 0.05$, resulting in

|Source|SS|Df|MS|E(MS)|$F_0$| P-Value |
|:------|:------|:-----:|:-------|:------|------:|------:|
|Area|$`r ssa`$ |`r a-1`|$`r ms[1]`$|$\sigma^2 + `r n`\sigma_{\gamma}^2 + `r c*n`\sigma_{\beta}^2 + `r b*c*n`\sigma_{\tau}^2$| $`r f[1]`$ |$`r fc[1]`$ |
|Lawn(Area)|$`r ssb`$| `r a*(b-1)`|$`r ms[2]`$|$\sigma^2 + `r n`\sigma_{\gamma}^2 + `r c*n`\sigma_{\beta}^2$|$`r f[2]`$|$`r fc[2]`$|
|Site(Lawn(Area))|$`r ssc`$| `r a*b*(c-1)`|$`r ms[3]`$|$\sigma^2 + `r n`\sigma_{\gamma}^2$|$`r f[3]`$|$`r fc[3]`$|
|Error|$`r sse`$| `r a*b*c*(n-1)` |$`r ms[4]`$|$\sigma^2$|          
|Total|$`r sst`$| `r (a*b*c*n)-1`|  

and we reject the null hypothesis that $\sigma_\tau^2 = 0$ and that $\sigma_\beta^2 = 0$   
  but we do not reject the hypothesis that $\sigma_\gamma^2 = 0$. 

## Appendix I  
### Showing My Work/R Code  

I did all the calculations and work using R studio and knitr (using LateX) in an R markdown file.  I display most the the coding here for reference.  

```{r, eval=F}
############ NUMBER 1 ############
# 2^3 design 
n <- 4; y <- c(425,426,1118,1283,1203,1396,1670,1807)
# build the design matrix
A <- c(-1,1,-1,-1,1,1,-1,1); B <- c(-1,-1,1,-1,1,-1,1,1)
C <- c(-1,-1,-1,1,-1,1,1,1);AB <- A*B; AC <- A*C; BC <- B*C; 
ABC <- A*B*C; dm <- cbind(A,B,C,AB,AC,BC,ABC)
con <- apply(dm, 2, FUN = function(x) sum(x*y)) 
effects <- con/16
# effects
id <- c("(1)", "a", "b", "c", "ab", "ac", "bc", "abc")
ss <- round((con^2)/32, 3) # sum of squares
# note, sst and sse cannot be estimated percisely
sst <- 466150; sse <- sst - sum(ss); mse <- sse/24 
f <- round(ss/mse,4); p <- round(pf(f,1,24, lower.tail = FALSE),5)
df1 <- data.frame(Factor = colnames(dm), effect = effects, 
                  Df = c(rep(1,7)),SS = ss, MS = ss, F0 = f, p)
rownames(df1) <- NULL
df3 <- df1[-2]
df3 <- rbind(df3, data.frame(Factor = c("Error", "Total"), Df = c(24,31), 
        SS = c(sse, sst), MS = c(mse, NA), F0 = c(NA, NA), 
        p = c(NA, NA)))
########### NUMBER 2 ###########
y <- c(86.3,84,85.8,86.1,85.2,87.3,88.5,87.3,89,89.4,
       89.9,90.3,89.1,90.2,101.3,91.7,93.2,98.7)
temp <- factor(c(rep("low",6), rep("med",6), rep("high",6)))
pressure <- factor(rep(c(250,260,270), 6))
df2 <- data.frame(y, temp, pressure)
df <- data.frame(y, temp = c(rep(-1,6), rep(0,6), rep(1,6)), 
                 pressure = rep(c(-1,0,1), 6))
fit <- aov(y ~ .*., df2)
# fitting the linear model
df$temp2 <- df$temp^2
df$pressure2 <- df$pressure^2
library(dplyr)
y <- df$y
y.i <- df %>%
  group_by(temp) %>%
  summarise_each(funs(sum), y)
y.j <- df %>%
  group_by(pressure) %>%
  summarise_each(funs(sum), y)
yij. <- df %>%
  group_by(temp, pressure) %>%
  summarise_each(funs(sum), y)
y.. <- sum(y)
a <- 3; b <- 3; n <- 2; N <- a*b*n
ssa <- (1/(b*n))*sum(y.i$y^2) - y..^2/N 
ssb <- (1/(a*n))*sum(y.j$y^2) - y..^2/N
sssub <- (1/n)*sum(yij.$y^2) - y..^2/N
ssab <- sssub - ssa - ssb
sst <- round(sum(y^2) - y..^2/N, 3)
sse <- sst - ssab - ssa - ssb
# now to add squared terms
row.sums <- df %>%
  group_by(temp, pressure) %>%
  summarise_each(funs(sum), y)
y <- row.sums$y
id <- c("00", "01", "02", "10", "11", "12", "20", "21", "22")
#data.frame(id, y) # these are our summaries..
#... now to develop the matrix
Al <- c(rep(-1,3), rep(0,3), rep(1,3))
Bl <- rep(c(-1,0,1),3)
Aq <- c(rep(1,3), rep(-2,3), rep(1,3))
Bq <- rep(c(1,-2,1), 3)
AlBl <- Al*Bl; AlBq <- Al*Bq; AqBl <- Aq*Bl; AqBq <- Aq*Bq
dm <- cbind(Al,Bl,Aq,Bq,AlBl,AlBq,AqBl,AqBq)
rownames(dm) <- id
m.effects <- apply(dm,2, FUN = function(d) sum(d*y)/2) # effects
# now for the sum of squares...
C <- apply(dm, 2, function(x) sum(x^2)) #contrasts
ss <- round(2*(m.effects^2)/C, 3) # sum of squares
ss2 <- round(cbind(ssa,ssb,ssab,sse,sst), 3)
sse <- round(sst - sum(ss),3); mse <- round(sse/9, 3)
############### NUMBER 3 ######################
flavor1 <- c(924,876,1150,1053,1041,1037,1125,1075,1066,977,886)
flavor2 <- c(891,982,1041,1135,1019,1093,994,960,889,967,838)
flavor3 <- c(817,1032,844,841,785,823,846,840,848,848,832)
df <- stack(data.frame(flavor1, flavor2, flavor3))
names(df) <- c("y", "flavor")
df$flavor <- as.factor(df$flavor)
fit <- aov(y ~ flavor, df)
msa <- summary(fit)[[1]][1,3]; mse <- round(summary(fit)[[1]][2,3],4); n <- 11
sig.tau <- round((msa-mse)/n,4)
a <- 3
cv <- qchisq(c(0.025, 1 - 0.025), (n*a) - a, lower.tail = FALSE) 
stat <- ((n*a - a)*mse) / cv
cv2 <- qf(c(0.025, 1 - 0.025), a-1, (a*n) - 1, lower.tail = FALSE)
L.U <- (1/n)*(((msa/mse)*(1/cv2)) - 1)
l.u <- L.U/(1 + L.U)
###########  NUMBER 4 ##############
# unrestricted mixed model for number 4
ssa <- 39.27; ssb <- 3935.96; ssab <- 48.51; sse <- 30.67 
sst <- 4054.4
a <- 3; b <- 10; n <- 3
msa <- ssa/(a-1); msb <- ssb/(b-1) 
msab <- ssab/((a-1)*(b-1)); mse <- sse/(a*b*(n-1))
f.op <- round(msa/msab,4); f.part <- round(msb/msab,4) 
f.inter <- round(msab/mse,4)
p.op <- round(1 - pf(f.op, a-1, (a-1)*(b-1)),5)
p.part <- round(1 - pf(f.part, b-1, (a-1)*(b-1)),5)
p.inter <- round(1 - pf(f.inter, (a-1)*(b-1), a*b*(n-1)),5)
##########  NUMBER 5  ##########
# nested model for number 5
ssa <- 1540; ssb <- 640; ssc <- 400; sse <- 680; sst <- 3260
SS <- c(1540,640,400,680,3260)
a <- 8; b <- 5; c <- 2; n <- 2
Factor <- c("Area", "Lawn(Area)", "Site(Lawn(Area))", "Error", "Total")
d <- c(a-1, a*(b-1), a*b*(c-1), a*b*c*(n-1), a*b*c*n - 1)
ms <- c(ssa/(a-1), ssb/(a*(b-1)), ssc/(a*b*(c-1)), sse/(a*b*c*(n-1)), NA)
t.2 <- data.frame(Factor,SS,df = d,ms)
f <- round(c(ms[1]/ms[2], ms[2]/ms[3], ms[3]/ms[4], NA, NA),4)
sig.tau <- (ms[1] - ms[2])/(b*c*n); sig.beta <- (ms[2] - ms[3])/(c*n)
sig.gam <- (ms[3] - ms[4])/n
fc <- round(pf(f, d[1:3], d[-1], lower.tail = FALSE),5)
t.3 <- cbind(t.2, F0 = f, p = fc)
```

