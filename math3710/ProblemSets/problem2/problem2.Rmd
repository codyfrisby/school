---
title: "Problem Set 2"
author: "Cody Frisby"
date: "January 23, 2016"
output: pdf_document
---

```{r, echo=FALSE}
# My professor doesn't want the code embedded in the document, but "pasted"
# at the end of the document. So i will paste all my code at the end with
# eval = FALSE.
agebp <- read.table("~/Documents/MATH3710/datafiles/AGEBP.DAT")
names(agebp) <- c("BP", "Age")
x <- agebp$Age; y <- agebp$BP
ssx <- sum((x - mean(x))^2)
ssy <- sum((y - mean(y))^2)
sxy <- sum((x-mean(x))*(y-mean(y)))
xbar <- mean(x); ybar <- mean(y)
corcoef <- sxy/sqrt(ssx*ssy)
# is SLR appropriate?
#plot(y ~ x, pch = 16)
# yes ^
#text(x=62, y=100, paste0("Cor = ", round(cor(x,y), 4)), cex = 0.7)
```


###  #1 Intent of the Researches?

I think the intent of the researches is to create a reliable model that allows them to not only predict blood pressure for people between `r min(x)` and `r max(x)` but also to have an understanding of perhaps a healthy range for a given age.  If there is a linear relationship between the variables of interest, a linear regression model would provide a simple and reliable way to predict and infer relationships between age and blood pressure.

### #2 Find SSY, SSX, SXY, $\bar x$, $\bar y$.

* ssx = $\sum(x^2)$ - $\frac {\sum(x)^2}{n} = `r sum(x^2)` - \frac{`r sum(x)^2`}{24}$  = `r ssx`
* ssy = $\sum(y^2)$ - $\frac {\sum(y)^2}{n} = `r sum(y^2)` - \frac{`r sum(y)^2`}{24}$ = `r ssy`
* sxy = $\sum(xy) - \frac{\sum(x)\sum(y)}{n}$ = $`r sum(x*y)` - \frac{`r sum(x)*sum(y)`}{24}$ = `r sxy`
* $\bar x$ = $\frac{\sum(x)}{n} = \frac{`r sum(x)`}{24}$ = `r xbar`
* $\bar y$ = $\frac{\sum(y)}{n} = \frac{`r sum(y)`}{24}$ = `r ybar`

### #3 Determine the correlation coefficient between x and y

$$ Cor(X,Y) = \frac {SXY}{\sqrt{SSX*SSY}} $$

$$ `r corcoef` = \frac {`r sxy`}{\sqrt{ `r ssx` * `r ssy` }} $$

### #4 Is SLR appropriate?

From the correlation coefficient and the scatter plot it appears that simple linear regression would be appropriate.

### #5 Least Squares regression Line

```{r, echo=FALSE}
b1 <- sxy/ssx
b0 <- mean(y) - (b1 * mean(x))
sse <- ssy - (sxy^2/ssx)
mse <- sse/(length(x) - 2)
rmse <- sqrt(mse)
# coefficient of determination
r <- corcoef^2
```

Equation of least squares regression line:
$$ Blood Pressure = `r b0` + `r b1` * Age  $$
The intercept, `r round(b0, 4)`, is computed $\bar y - \bar x * \hat \beta_1$ 
and is interpreted as the mean blood pressure when someone's age is 0. $\hat \beta_1$, the coefficient to age, is equal to `r round(b1, 4)` and is computed $\frac {sxy^2}{ssx}$.  This is to be interpreted that for every unit increase in age there will be an approximate `r round(b1,3)` increase in blood pressure, *on average*.

### #6 Estimated common standard deviation for sub populations:

This can be calculated by the formula $$\hat \sigma = \sqrt{\frac {SSE}{n-2}}$$
Where $SSE = SSY - \frac {SXY^{2}}{SSX}$

So for our data $\hat \sigma = \sqrt{\frac {`r ssy` - \frac {33699476}{3608.9583}}{22}}$  = `r round(rmse,6)`

### #7 Coefficient of determination:

This can be calculated from the sample data a couple of ways.  Its value is always between 0 and 1.  Its interrpretation is the porportion of the variation in the response variable that can be explained by the predictor variable.  Above, where we calculated to coefficient of correlation, we can use the square of it's value to obtain the coefficient of determination.  So, $`r corcoef`^{2}$ = `r corcoef^2` is the value for the coefficient of determination, also represented by $r^{2}$.  So, approximately 98% of the variation in blood pressure can be explained by age.

### #8 Mean blood pressure for age 52.

To find the mean blood pressure for someone who is 52 we would simply plug 52 in for age into our least squares equation  $$ Blood Pressure = `r b0` + `r b1` * Age  $$
$$ `r b1*52 + b0` = `r b0` + `r b1` * 52 $$

### #9 Discuss Assumptions
* Linearity:  
The assumption of linearity is valid.  There appears to be a strait line relationship between the two variables.  There are not any outliers that we need to be concered about.  
* Equal Variances:  
This assumption involves assuming that the variance, or standard deviations, for the sub populations are approximately equal.  Graphically this entails that we have random scatter of our models standardized residuals about a horizontal line at zero.  This can be a subjective exercise, but for our assumption to be valid, we do not want to see any obvious patterns or funneling.  It appears by this plot that the homogeneity of variances is valid.  
* Normality:  
This assumption involves that each subpopulation of blood pressure determined by age is a Gaussian population.  One way to graphically investigate this is to create a qqplot.  If this plot falls on an approximate strait line with few points straying off this strait line then our assumption of normality is valid.  From the qqplot on the printout it appears this assumption is valid.  Another plot is the histogram of the residuals.  This one looks ok too.  The data looks approx. Gaussian.  

### #10 For people 52 years old find a 95% confidence interval for blood pressure:  

```{r, echo=FALSE}
fit <- lm(y ~ x)
prebp <- predict.lm(fit, newdata = data.frame(x=c(52, 42)), 
                    interval = "prediction", level = 0.95)
confbp <- predict.lm(fit, newdata = data.frame(x=c(52, 42)), 
                     interval = "confidence", level = 0.95)
tt <- qt(0.975, df = length(x) - 2)
semux <- rmse*sqrt((1/length(x)) + (52 - xbar)^2/ssx)
seyx <- rmse*sqrt(1 + (1/length(x)) + (52 - xbar)^2/ssx)
yhat52 <- prebp[1,1]
yhat42 <- prebp[2,1]
```

From number 8 above we showed that the predicted value of blood pressure when age is 52 = `r confbp[1,1]`.  Now to calculate a 95% confidence interval.  The general form for our least square regression paramater estimators is given by:
$$\hat \theta - (table value) * SE(\hat \theta) \leq \theta \leq \hat \theta + 
(table value) * SE(\hat \theta)$$

We calculate $SE(\hat \mu(x))$ by the following:
$$SE(\hat \mu(x)) = \hat \sigma \sqrt{\frac{1}{n} + \frac{( x - \bar{x})^2}{SSX}}$$  

$$`r semux` = `r rmse` \sqrt{\frac{1}{24} + \frac{(52 - 44.95833)^2}{`r ssx`}}$$  
And our t statistic, with 22 degrees of freedom and $\alpha = 0.05$, = `r round(tt,4)`.  Now plugging in $\hat \mu(x)$, t statistic, and $SE(\hat \mu(x))$ to our general form confidence interval we get:

$$`r yhat52` - `r tt` * `r semux` \leq \mu(52) \leq `r yhat52` + `r tt` * `r semux` $$ 

$$`r yhat52 - tt * semux` \leq \mu(52) \geq `r yhat52 + tt * semux` $$ 

$$[`r confbp[1,2]`, `r confbp[1,3]`]$$  

The 95% confidence interval for x = 52


### #11 Find a 95% prediction interval:

From number 8 above we showed that the predicted value of blood pressure when age is 52 = `r prebp[1,1]`.  Now to calculate a 95% prediction interval:

$$SE(\hat Y(x)) = \hat \sigma \sqrt{ 1 + \frac{1}{n} + \frac{( x - \bar{x})^2}{SSX}}$$  

$$`r seyx` = `r rmse` \sqrt{1 + \frac{1}{24} + \frac{(52 - 44.95833)^2}{`r ssx`}}$$

$$`r yhat52` - `r tt` * `r seyx` \leq Y(52) \leq `r yhat52` + `r tt` * `r seyx` $$ 

$$`r yhat52 - tt * seyx` \leq Y(52) \leq `r yhat52 + tt * seyx` $$ 

95% prediction interval for x = 52  
$$[`r prebp[1,2]`, `r prebp[1,3]`]$$

### #12 Find 95% confidence interval for $\beta_1$

```{r, echo=FALSE}
# calculations and code needed for no.12.
seb1 <- rmse/sqrt(ssx)
```

Our estimate for $\beta_1$ is `r b1`. The standard error for $\hat \beta_1$ is:

$$SE(\hat \beta_1) = \frac {\hat \sigma}{\sqrt{SSX}}$$

So for our data $SE(\hat \beta_1)$ = $\frac {`r round(rmse,4)`}{\sqrt{`r round(ssx,4)`}}$ = `r round(seb1,4)`.  And our t statistic, with 22 degrees of freedom and $\alpha = 0.05$, = `r round(tt,4)`

$$\hat \beta_1 - `r round(tt,4)` * SE(\hat \beta_1) \leq \beta_1 \leq \hat \beta_1 + `r round(tt,4)` * SE(\hat \beta_1)$$

So, the confidence interval is calculated by plugging into the above equation:
 
$$`r b1` - `r tt` * `r seb1` \leq \beta_1 \leq `r b1` + `r tt` * `r seb1` $$ 

95% confidence interval for $\beta_1$:  
$$[`r confint(fit)[2,1]`, `r confint(fit)[2,2]`]$$

### #13 Find simultaneous confidence intervals for x=42 and x=52

The interval formulation is the same as above:  
$$\hat \theta - (table value) * SE(\hat \theta) \leq \theta \leq \hat \theta + (table value) * SE(\hat \theta)$$  
The table value will be different since we want simultaneous confidence for m = 2.  Looking up the value using the table on pg. 684 in the text book with 0.95 confidence and df=22 we find 2.405.  
First, for x = 52:
$$`r yhat52` - 2.405 * `r semux` \leq \mu(52) \leq `r yhat52` + 2.405 * `r semux` $$ 
$$`r yhat52 - 2.405 * semux` \leq \mu(52) \leq `r yhat52 + 2.405 * semux` $$  

And for x = 42:
$$`r yhat42` - 2.405 * `r semux` \leq \mu(42) \leq `r yhat42` + 2.405 * `r semux` $$ 
$$`r yhat42 - 2.405 * semux` \leq \mu(42) \leq `r yhat42 + 2.405 * semux` $$

So, the 95% simultaneous confidence interval is:  
$$[`r yhat42 - 2.405 * semux`, `r yhat42 + 2.405 * semux`~~and~~`r yhat52 - 2.405 * semux`, `r yhat52 + 2.405 * semux`]$$

### #14 How useful is this model?

```{r, echo = FALSE, comment=NA}
# to calculate 99% prediction intervals for all individuals.
good <- predict.lm(fit, newdata = data.frame(x=23:67), 
                   interval = "confidence", level = 0.99)
good <- data.frame(good)
good$diff <- good$upr - good$lwr
# or would we run a test on the residual standard error?
# or
test <- summary(fit)
# Or compute a confidence interval for sigma using the chi-squared distribution
chi1 <- qchisq(0.95, df=22)
chi2 <- qchisq(0.05, df=22)
```

The Residual standard error from our model is `r test$sigma`. If $\mu_y(x) = x$ then 99% of individuals that are within $\pm$ 8 (the max distance from the mean for the Dr) of the average will be between $\mu_y(x) - z_{0.995} \sigma ~~ and ~~ \mu_y(x) + z_{0.995} \sigma$.  So, $`r qnorm(0.995)` * `r test$sigma` = 7.304059$.  And a confidence interval: $$[\hat \mu(x) - 7.304059, ~\hat \mu(x) + 7.304059]$$  This is within the 8 value that the Dr is interested in, and, assuming robustness of our asumptions, we conclude that the model will be useful to him/her.  

***Note: I got this one wrong so I am going to edit it here***  
First we need to calculate a confidence interval for $\sigma$.  The formula for this is as follows  
$$C \Bigg[\sqrt{\frac{(df)\hat \sigma^2}{\chi_{1-\alpha:df}^2}} \leq \sigma \leq \sqrt{\frac{(df)\hat \sigma^2}{\chi_{\alpha/2:df}^2}}~ \Bigg] = 1 - \alpha$$  

$$C \Bigg[\sqrt{\frac{22 \times `r mse`}{`r chi1`}} \leq \sigma \leq \sqrt{\frac{22 \times `r mse`}{`r chi2`}} ~ \Bigg] = `r 1-0.10`$$  
90% confidence interval for $\sigma$  $$[`r sqrt((22*mse)/chi1)`, `r sqrt((22*mse)/chi2)`]$$  
Now using the z score from above and that we are 95% confident that $\sigma \leq `r sqrt((22*mse)/chi2)`$ we can say that $`r qnorm(0.995)` \times `r sqrt((22*mse)/chi2)`$ = `r qnorm(0.995)*sqrt((22*mse)/chi2)`.  We cannot conclude that the model will be useful to the Dr.  

### #15 Conclusions
This experiment appears to have genterated data that built a very good linear model.  By good I mean one with small errors.  It seems like this could be of benefit to the medical community in that it should predict, with minimal error, where one should be given their age and if they are not perhaps identify potential health risks with the individual.  Additionaly, this study should lead to other studies where we confirm or modify our findings.